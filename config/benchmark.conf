# LLM Performance Benchmark Configuration
# This file contains default parameters for benchmark runs

[default]
# Number of samples to run for each test
samples = 100

# Token configuration
prompt_tokens = 1000
output_tokens = 800

# Request configuration
max_requests = 100
rate = 1

# Load profile (constant, ramp, burst)
profile = "constant"

# Output formats (json, csv, html)
outputs = "json,csv,html"

# Backend configuration
validate_backend = false

# Logging configuration
console_log_level = "INFO"
file_log_level = "DEBUG"
log_file = "benchmark.log"

[performance]
# High-throughput test settings
[performance.high_throughput]
samples = 500
max_requests = 1000
rate = 10
prompt_tokens = 500
output_tokens = 200

[performance.low_latency]
samples = 50
max_requests = 100
rate = 0.5
prompt_tokens = 100
output_tokens = 50

[models]
# Model-specific configurations
[models.small]
prompt_tokens = 500
output_tokens = 400
samples = 200

[models.medium]
prompt_tokens = 1000
output_tokens = 800
samples = 100

[models.large]
prompt_tokens = 2000
output_tokens = 1500
samples = 50

[models.xlarge]
prompt_tokens = 4000
output_tokens = 3000
samples = 25

[test_scenarios]
# Predefined test scenarios
[test_scenarios.quick_test]
description = "Quick validation test"
samples = 10
max_requests = 20
prompt_tokens = 100
output_tokens = 100
rate = 1

[test_scenarios.standard]
description = "Standard performance test"
samples = 100
max_requests = 200
prompt_tokens = 1000
output_tokens = 800
rate = 2

[test_scenarios.stress_test]
description = "High-load stress test"
samples = 1000
max_requests = 2000
prompt_tokens = 2000
output_tokens = 1500
rate = 20